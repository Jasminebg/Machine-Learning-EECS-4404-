{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4404 Project",
      "provenance": [],
      "collapsed_sections": [
        "3ZKB-hQyPuQ3",
        "1EFUpOmieeFK",
        "4VdnAZNTL-Gk",
        "GT2AUmzaMAZU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZKB-hQyPuQ3"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeGPsGUjdQA0",
        "outputId": "cc45f9a1-67fe-4b4a-8f89-83be77701c43"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import scipy\n",
        "from scipy.sparse import linalg as splinalg\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import tracemalloc\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import copy\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EFUpOmieeFK"
      },
      "source": [
        "#Document Word Matrix Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17vgCSD6Ie_k",
        "outputId": "780de0cc-9760-49cc-9610-e12e40863234"
      },
      "source": [
        "#all unique words and list of docs in corpus\n",
        "def Unique():\n",
        "    #enwiki8 doc\n",
        "    #Getting count of all words\n",
        "    uniquecount = {}\n",
        "    documents = []\n",
        "    with open('/content/drive/My Drive/enwiki8.txt', 'r',encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "          #wordfreq = {}\n",
        "          for word in line.strip().split():\n",
        "            #if word not in stopwords:\n",
        "            if word in uniquecount:\n",
        "              uniquecount[word] +=1\n",
        "            else:\n",
        "              uniquecount[word] = 1\n",
        "          #document = list(wordfreq.values())\n",
        "          documents.append(line.strip().lower())\n",
        "    return uniquecount, documents\n",
        "#Top10k~ words + humanscores\n",
        "def Top10k(terms,stopwords):\n",
        "    #Getting top 10k words\n",
        "    #print(len(top10k))\n",
        "    #print(top10k)\n",
        "    humanscores = []\n",
        "    hwords = {}\n",
        "    with open(\"/content/drive/My Drive/wordsim353_human_scores.txt\",'r',encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            humanscores.append(line.strip().split())\n",
        "            word1, word2, sc = line.strip().split()\n",
        "            if(word1 in uniquecount and word1 not in hwords):\n",
        "              hwords[word1] = uniquecount[word1]\n",
        "              #top10k[word1] = uniquecount[word1]\n",
        "            if(word2 in uniquecount and word2 not in hwords):\n",
        "              hwords[word2] = uniquecount[word2]   \n",
        "              #top10k[word2] = uniquecount[word2]           \n",
        "      \n",
        "    for x in terms.keys():\n",
        "      if x in stopwords and x not in hwords:\n",
        "        terms[x] = 0          \n",
        "    top10k = dict(sorted(terms.items(), key=lambda x:x[1], reverse=True)[:10000])\n",
        "    for word in hwords:\n",
        "      top10k[word] = hwords[word]\n",
        "    top10k = dict(sorted(top10k.items(), key=lambda x:x[1], reverse=True))\n",
        "    #assigning row to each word in the top 10k words\n",
        "    return top10k, {k:v for v,k in enumerate(list(top10k.keys()))}, humanscores\n",
        "\n",
        "\n",
        "starttime = time.time()\n",
        "cv = CountVectorizer(stop_words=\"english\",max_features=10000)\n",
        "stop = cv.get_stop_words()\n",
        "print(\"Part 1 started... \")\n",
        "uniquecount,docs = Unique()\n",
        "print(\"Unique count of all terms obtained\")\n",
        "top10k, wordrows, humansc = Top10k(uniquecount,stop)\n",
        "cv.stop_words=None\n",
        "cv.vocabulary=top10k.keys()\n",
        "print(\"Top 10,000 terms and row# for all words obtained\")\n",
        "tf = TfidfTransformer()\n",
        "docwords = cv.fit_transform(docs)\n",
        "#dwf = (tf.fit_transform(docwords)).astype(float).T\n",
        "dwf = csr_matrix(docwords).astype(float).T\n",
        "print(\"Sparse Documemt Word Frequency Matrix obtained, shape of matrix: \")\n",
        "print(dwf.shape)\n",
        "print(scipy.sparse.issparse(dwf))\n",
        "print(\"Part 1 completed\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Part 1 started... \n",
            "Unique count of all terms obtained\n",
            "Top 10,000 terms and row# for all words obtained\n",
            "Sparse Documemt Word Frequency Matrix obtained, shape of matrix: \n",
            "(10076, 489860)\n",
            "True\n",
            "Part 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcZ7YiD3Q-MH",
        "outputId": "6ddc9b34-5b28-4300-f2fe-025656df19f9"
      },
      "source": [
        "#Obtaining a list of indices, seperated so it did not slow down testing\n",
        "indices = list(zip(*dwf.nonzero()))\n",
        "print(\"List of nonzero indices obtained.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of nonzero indices obtained.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VdnAZNTL-Gk"
      },
      "source": [
        "#PPMI Alteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9SPK16_QkcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c505a44a-a0f7-4905-d5f3-01111905a5d2"
      },
      "source": [
        "#PPMI\n",
        "#To reset values from TFIDF to normal\n",
        "dwf = csr_matrix(docwords).astype(float).T\n",
        "f = np.empty(dwf.shape)\n",
        "rowsums = np.sum(dwf,axis=1)\n",
        "colsums = np.sum(dwf,axis=0)\n",
        "print(colsums.shape)\n",
        "print(rowsums.shape)\n",
        "dwfsum = np.sum(dwf)\n",
        "#dwfsum = np.sum(rowsums)\n",
        "#sum of all columns in rows i\n",
        "pi = colsums/dwfsum\n",
        "#sum of all rows in columns j\n",
        "pj = rowsums / dwfsum\n",
        "for r,c in indices:\n",
        "    pij = dwf[r,c]/dwfsum\n",
        "    ppmi = np.log(pij/(pi[:,c]*pj[r]))\n",
        "    dwf[r,c] = max(0,ppmi)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 489860)\n",
            "(10076, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2AUmzaMAZU"
      },
      "source": [
        "#SVD Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSUuHaLCz8B3",
        "outputId": "57593734-74c0-4e81-a7e4-2a8ff4af9ce7"
      },
      "source": [
        "#2 Factorize with SVD\n",
        "print(\"Part 2 started... \")\n",
        "for k in (20,50,100):\n",
        "    #tracemalloc.start()\n",
        "    starttime = time.time()\n",
        "    svdu,svds,svdvh= splinalg.svds(dwf, k=k)\n",
        "\n",
        "    print(\"When K = \" + str(k) + \":\" )\n",
        "    print(\"U shape: \", svdu.shape)\n",
        "    print(\"S shape: \",svds.shape)\n",
        "    print(\"V shape: \",svdvh.shape)\n",
        "    #print(\"s:\")\n",
        "    #print(s)\n",
        "    runtime = time.time()-starttime\n",
        "    print(\"Runtime: \" + str(runtime))\n",
        "    humandistances = []\n",
        "    svddistances = []\n",
        "    #computing Pearson correlation coefficient between cosine distances and human scores.\n",
        "    for word1, word2, score in humansc:\n",
        "      #print(word1,word2,score)\n",
        "      w1row = wordrows[word1] \n",
        "      w2row = wordrows[word2]\n",
        "      humanscore = float(score)\n",
        "      sw1 = svdu[w1row]\n",
        "      sw2 = svdu[w2row]\n",
        "      sdistance = np.abs(sw1.dot(sw2.T)) / np.abs(np.linalg.norm(sw1)*np.linalg.norm(sw2))\n",
        "      humandistances.append(humanscore)\n",
        "      svddistances.append(sdistance)\n",
        "    print(\"PCC: \", np.corrcoef(humandistances,svddistances)[0][1])\n",
        "\n",
        "print(\"Part 2 completed\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Part 2 started... \n",
            "When K = 20:\n",
            "U shape:  (10076, 20)\n",
            "S shape:  (20,)\n",
            "V shape:  (20, 489860)\n",
            "Runtime: 3.622018814086914\n",
            "PCC:  0.3735864469048904\n",
            "When K = 50:\n",
            "U shape:  (10076, 50)\n",
            "S shape:  (50,)\n",
            "V shape:  (50, 489860)\n",
            "Runtime: 6.775550842285156\n",
            "PCC:  0.4404108169693773\n",
            "When K = 100:\n",
            "U shape:  (10076, 100)\n",
            "S shape:  (100,)\n",
            "V shape:  (100, 489860)\n",
            "Runtime: 14.677078247070312\n",
            "PCC:  0.5123596681353468\n",
            "Part 2 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9iaO0ljMD0T"
      },
      "source": [
        "#SGD Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hhwd5vivgGP",
        "outputId": "4a3f9f24-bdf2-481a-ef03-17c91ca77544"
      },
      "source": [
        "print(\"Part 3 started.\")\n",
        "krates = {20:5e-2,50:5e-2,100:1e-2}\n",
        "regs =  {20:1e-1,50:1e-2,100:1e-3}\n",
        "kconv = {20:(1.1,0.97), 50:(1.07,0.99), 100:(1.1,0.95)}\n",
        "#convs = [(1.01,0.98), (1.0025,0.99)]\n",
        "#convs = (1.02,0.98)\n",
        "np.seterr('raise')\n",
        "for k in (20,50,100):\n",
        "    #for ratemult in convs:\n",
        "    v = np.random.rand(k,dwf.shape[1])\n",
        "    u = np.random.rand(dwf.shape[0], k)\n",
        "    starttime = time.time()\n",
        "    convs = (1.05,0.95)\n",
        "    loss = [1e10]\n",
        "    cont = True\n",
        "    lambdaID = 1e-5* np.ones((k,k))\n",
        "    rate = krates[k]\n",
        "    t=0\n",
        "    while cont:\n",
        "        # #SVD \n",
        "        errsum = 0\n",
        "        for num in range(5000):\n",
        "          try:\n",
        "            r,c =  indices[np.random.choice(len(indices))]\n",
        "            r=num\n",
        "            err = (u[r].dot(v[:,c]) -  dwf[r,c])\n",
        "            #delaying the update to u[r] so v[:,c ] can be updated with the unchanged u[r]\n",
        "            tmpur = u[r] - rate *(err*v[:,c]-  lambdaID.dot(u[r] ))\n",
        "            v[:,c] -= rate * (err*u[r]  - lambdaID.dot(v[:,c] ) )\n",
        "            u[r] = tmpur\n",
        "            errsum += err**2\n",
        "          except:\n",
        "            # print(err,errsum,rate)\n",
        "            cont = False\n",
        "        t+=1\n",
        "        loss.append(np.sqrt(errsum/(num+1)))\n",
        "        rate *= convs[0] if loss[-1] < loss[-2] else   convs[1]\n",
        "        # if t%5 == 0:\n",
        "          # print(t,loss[-2:],rate, np.mean(loss))\n",
        "        \n",
        "        if np.abs(loss[-1]-loss[-2])  < 1e-2 :\n",
        "          cont = False\n",
        "          break\n",
        "        if loss[0] == 1e10:\n",
        "          del(loss[0])\n",
        "    runtime = time.time()-starttime\n",
        "    # if np.mean(loss[-6:]) > 100:\n",
        "    #print(loss[-6:])\n",
        "    print(\"When K = \" + str(k) + \":\" )\n",
        "    print(\"U shape: \", u.shape)\n",
        "    print(\"V shape: \" , v.shape)\n",
        "    print(\"Runtime: \" ,runtime)\n",
        "    print(\"Iterations: \", t)\n",
        "    #print(rate)\n",
        "    \n",
        "    distances = []\n",
        "    humandistances = []\n",
        "    svddistances = []\n",
        "    for word1, word2, score in humansc:\n",
        "      #print(word1,word2,score)\n",
        "      w1row = wordrows[word1] \n",
        "      w2row = wordrows[word2]\n",
        "      humanscore = float(score)\n",
        "      w1 = u[w1row]\n",
        "      w2 = u[w2row]\n",
        "      distance = np.abs(w1.dot(w2.T)) / np.abs(np.linalg.norm(w1)*np.linalg.norm(w2))\n",
        "      humandistances.append(humanscore)\n",
        "      distances.append(distance)\n",
        "    #print(np.std(humandistances),np.std(distances))\n",
        "    print(\"PCC: \", np.corrcoef(humandistances,distances)[0][1],\"\\n\")\n",
        "print(\"Part 3 and 4 completed\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Part 3 started.\n",
            "When K = 20:\n",
            "U shape:  (10076, 20)\n",
            "V shape:  (20, 489860)\n",
            "Runtime:  4.481069564819336\n",
            "Iterations:  13\n",
            "PCC:  0.1031895798974619 \n",
            "\n",
            "When K = 50:\n",
            "U shape:  (10076, 50)\n",
            "V shape:  (50, 489860)\n",
            "Runtime:  2.41158390045166\n",
            "Iterations:  7\n",
            "PCC:  0.09299918201948869 \n",
            "\n",
            "When K = 100:\n",
            "U shape:  (10076, 100)\n",
            "V shape:  (100, 489860)\n",
            "Runtime:  7.928662061691284\n",
            "Iterations:  15\n",
            "PCC:  0.10087177143856578 \n",
            "\n",
            "Part 3 and 4 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAIXobtvRKz4"
      },
      "source": [
        "#T-SNE Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ObAwHDGJ9GJ"
      },
      "source": [
        "print(\"Part 5 started... \")\n",
        "#graphing\n",
        "top300 = list(wordrows.items())[:300]\n",
        "vecs = []\n",
        "wordkeys = []\n",
        "for word,row in top300:\n",
        "  vecs.append(u[row])\n",
        "  wordkeys.append(word)\n",
        "vecs = np.asarray(vecs)\n",
        "plt.figure(figsize=(20, 15), dpi=80)\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "np.set_printoptions(suppress=True)\n",
        "Y = tsne.fit_transform(vecs)\n",
        "x_coords = Y[:, 0]\n",
        "y_coords = Y[:, 1]\n",
        "plt.scatter(x_coords, y_coords)\n",
        "for label, x, y in zip(wordkeys, x_coords, y_coords):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "plt.show()\n",
        "\n",
        "print(\"Part 5 completed\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}